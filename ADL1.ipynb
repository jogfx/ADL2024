{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jogfx/ADL2024/blob/main/ADL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EmaLAjqmG6nU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pyN9vLDMIJSX"
      },
      "outputs": [],
      "source": [
        "# Define transformations to be applied to the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kVT1R567ILuO"
      },
      "outputs": [],
      "source": [
        "# Choose CIFAR-10 or CIFAR-100\n",
        "dataset_name = \"CIFAR-10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il1xDsy1INLm",
        "outputId": "b071bf77-7f1f-48e6-efb2-a798a7a13fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 39739003.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "# Download and load the training set\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCMWnXBiIOp6",
        "outputId": "f5c29129-9d49-4ff9-ef86-01a22d37a6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download and load the test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L4YUho5R4unF"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVX2L9Im4unF",
        "outputId": "92275b73-2241-47ce-f02e-b987bd947ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Create the training set with data augmentation\n",
        "trainset_augmented = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU-mzq0UIP5i",
        "outputId": "2acb8c5f-e758-47e4-814e-04630ed57086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "# Print the classes for reference\n",
        "classes = trainset.classes\n",
        "print(f'Classes: {classes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CUD-veXlPHD4"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter variations\n",
        "hidden_size1_values = [64, 128, 256, 512]\n",
        "hidden_size2_values = [64, 128, 256, 512]\n",
        "hidden_size3_values = [64, 128, 256, 512]\n",
        "learning_rate_values = [0.008, 0.02, 0.005]\n",
        "batch_size_values = [32, 64, 128]\n",
        "epochs = [10, 20, 30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EAW3aXKaPHiC"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "input_size = 3 * 32 * 32  # CIFAR-10 with 3 channels and 32x32 pixels\n",
        "output_size =  10  # number of classes in cifar10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jnIz1OmqJyFQ"
      },
      "outputs": [],
      "source": [
        "# Number of random experiments\n",
        "num_experiments = 5\n",
        "num_epochs = random.choice(epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dsEx_Z9hRMKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd11540d-a053-489c-c4de-c6e399825ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1: Hidden Size 1=512, Hidden Size 2=64, Hidden Size 3=512, Learning Rate=0.02, Epoch=20\n",
            "Test Accuracy: 0.5664\n",
            "Experiment 2: Hidden Size 1=256, Hidden Size 2=64, Hidden Size 3=512, Learning Rate=0.005, Epoch=20\n",
            "Test Accuracy: 0.5755\n",
            "Experiment 3: Hidden Size 1=64, Hidden Size 2=256, Hidden Size 3=256, Learning Rate=0.02, Epoch=20\n",
            "Test Accuracy: 0.5496\n",
            "Experiment 4: Hidden Size 1=256, Hidden Size 2=512, Hidden Size 3=256, Learning Rate=0.005, Epoch=20\n",
            "Test Accuracy: 0.5813\n",
            "Experiment 5: Hidden Size 1=256, Hidden Size 2=128, Hidden Size 3=512, Learning Rate=0.02, Epoch=20\n",
            "Test Accuracy: 0.5625\n"
          ]
        }
      ],
      "source": [
        "for experiment_num in range(num_experiments):\n",
        "    # Randomly select hyperparameters for this experiment\n",
        "    hidden_size1 = random.choice(hidden_size1_values)\n",
        "    hidden_size2 = random.choice(hidden_size2_values)\n",
        "    hidden_size3 = random.choice(hidden_size3_values)\n",
        "    lr = random.choice(learning_rate_values)\n",
        "\n",
        "    # Model structure\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size1),\n",
        "        nn.BatchNorm1d(hidden_size1),\n",
        "        nn.LeakyReLU(0.01),\n",
        "        nn.Linear(hidden_size1, hidden_size2),\n",
        "        nn.BatchNorm1d(hidden_size2),\n",
        "        nn.LeakyReLU(0.01),\n",
        "        nn.Linear(hidden_size2, hidden_size3),\n",
        "        nn.BatchNorm1d(hidden_size3),\n",
        "        nn.LeakyReLU(0.01),\n",
        "        nn.Linear(hidden_size3, output_size)\n",
        "    )\n",
        "\n",
        "    # Add a custom weight initialization function\n",
        "    def weights_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    # Apply the custom initialization to the model\n",
        "    model.apply(weights_init)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(trainset_augmented, batch_size=random.choice(batch_size_values), shuffle=True)\n",
        "    test_loader = DataLoader(testset, batch_size=random.choice(batch_size_values), shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        for inputs, labels in train_loader:\n",
        "          inputs = inputs.view(inputs.size(0), -1)  # Flatten each input\n",
        "          optimizer.zero_grad()  # Zero the gradients\n",
        "          outputs = model(inputs)  # Forward pass\n",
        "          loss = criterion(outputs, labels)  # Compute the loss\n",
        "          loss.backward()  # Backward pass\n",
        "          optimizer.step()  # Update weights\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Experiment {experiment_num + 1}: Hidden Size 1={hidden_size1}, Hidden Size 2={hidden_size2}, Hidden Size 3={hidden_size3}, Learning Rate={lr}, Epoch={epoch+1}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}